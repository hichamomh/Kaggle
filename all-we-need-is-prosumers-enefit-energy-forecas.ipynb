{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":57236,"databundleVersionId":7230081,"sourceType":"competition"},{"sourceId":6924580,"sourceType":"datasetVersion","datasetId":3976011},{"sourceId":7140010,"sourceType":"datasetVersion","datasetId":3933894}],"dockerImageVersionId":30559,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"border-radius: 10px; border: #babab5 solid; padding: 15px; background-color: orange; font-size: 100%;\">\n\n\nThe competition's description, data details, and structure are all drawn from Rafi Hai's notebook. I encourage you to review and upvoting ‚¨ÜÔ∏è‚¨ÜÔ∏è **Rafi Hai's** notebook (https://www.kaggle.com/code/rafiko1/enefit-xgboost-starter) for further insights. I extend my heartfelt gratitude to the author for their previous contributions.\n\nüìå **Note**: If you found this notebook helpful, kindly consider upvoting ‚¨ÜÔ∏è‚¨ÜÔ∏è.","metadata":{}},{"cell_type":"markdown","source":"# Introduction \n> üìå**Note**: If you liked or forked this notebook, please consider upvoting ‚¨ÜÔ∏è‚¨ÜÔ∏è It encourages to keep posting relevant content\n\n<div style=\"border-radius:10px; border: #babab5 solid; padding: 15px; background-color: #e6f9ff; font-size:100%; \">\n    \nThis notebook covers the following:\n* EAD and Pre-processing of the different datasets\n* Our strategy\n* feature engineering \n* Modelisation ","metadata":{}},{"cell_type":"markdown","source":"# Competition Description\n<img src =\"https://www.energy.gov/sites/default/files/styles/full_article_width/public/Prosumer-Blog%20sans%20money-%201200%20x%20630-01_0.png?itok=2a3YSkUb\" width=600>\n\n> üìå**Note**:  Energy prosumers are individuals, businesses, or organizations that both consume and produce energy. This concept represents a shift from the traditional model where consumers simply purchase energy from utilities and rely on centralized power generation sources. Energy prosumers are actively involved in the energy ecosystem by generating their own electricity, typically through renewable energy sources like solar panels (or wind turbines, small-scale hydropower etc.). They also consume energy from the grid when their own generation is insufficient to meet their needs\n\n<div style=\"border-radius:10px; border: #babab5 solid; padding: 15px; background-color: #e6f9ff; font-size:100%; \">\n    \n* The number of prosumers is rapidly increasing, associated with higher energy imbalance - increased operational costs, potential grid instability, and inefficient use of energy resources.\n* The goal of the competition is to create an energy prediction model of prosumers to reduce energy imbalance costs\n* If solved, it would reduce the imbalance costs, improve the reliability of the grid, and make the integration of prosumers into the energy system more efficient and sustainable.\n*  Moreover, it could potentially incentivize more consumers to become prosumers and thus promote renewable energy production and use.","metadata":{}},{"cell_type":"markdown","source":"# Data Description\n> üìå**Note**:  Your challenge in this competition is to predict the amount of electricity produced and consumed by Estonian energy customers who have installed solar panels. You'll have access to weather data, the relevant energy prices, and records of the installed photovoltaic capacity. <br> <br>\nThis is a forecasting competition using the time series API. The private leaderboard will be determined using real data gathered after the submission period closes.\n\n<div style=\"border-radius:10px; border: #babab5 solid; padding: 15px; background-color: #e6f9ff; font-size:100%; \">\n\n## Files\n\n**train.csv**\n\n- `county` - An ID code for the county.\n- `is_business` - Boolean for whether or not the prosumer is a business.\n- `product_type` - ID code with the following mapping of codes to contract types: `{0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}`.\n- `target` - The consumption or production amount for the relevant segment for the hour. The segments are defined by the `county`, `is_business`, and `product_type`.\n- `is_consumption` - Boolean for whether or not this row's target is consumption or production.\n- `datetime` - The Estonian time in EET (UTC+2) / EEST (UTC+3).\n- `data_block_id` - All rows sharing the same `data_block_id` will be available at the same forecast time. This is a function of what information is available when forecasts are actually made, at 11 AM each morning. For example, if the forecast weather `data_block_id` for predictions made on October 31st is 100 then the historic weather `data_block_id` for October 31st will be 101 as the historic weather data is only actually available the next day.\n- `row_id` - A unique identifier for the row.\n- `prediction_unit_id` - A unique identifier for the `county`, `is_business`, and `product_type` combination. _New prediction units can appear or dissappear in the test set_.\n\n**gas\\_prices.csv**\n\n- `origin_date` - The date when the day-ahead prices became available.\n- `forecast_date` - The date when the forecast prices should be relevant.\n- `[lowest/highest]_price_per_mwh` - The lowest/highest price of natural gas that on the day ahead market that trading day, in Euros per megawatt hour equivalent.\n- `data_block_id`\n\n**client.csv**\n\n- `product_type`\n- `county` - An ID code for the county. See `county_id_to_name_map.json` for the mapping of ID codes to county names.\n- `eic_count` - The aggregated number of consumption points (EICs - European Identifier Code).\n- `installed_capacity` - Installed photovoltaic solar panel capacity in kilowatts.\n- `is_business` - Boolean for whether or not the prosumer is a business.\n- `date`\n- `data_block_id`\n\n**electricity\\_prices.csv**\n\n- `origin_date`\n- `forecast_date`\n- `euros_per_mwh` - The price of electricity on the day ahead markets in euros per megawatt hour.\n- `data_block_id`\n\n**forecast\\_weather.csv** Weather forecasts that would have been available at prediction time. Sourced from the [European Centre for Medium-Range Weather Forecasts](https://codes.ecmwf.int/grib/param-db/?filter=grib2).\n\n- `[latitude/longitude]` - The coordinates of the weather forecast.\n- `origin_datetime` - The timestamp of when the forecast was generated.\n- `hours_ahead` - The number of hours between the forecast generation and the forecast weather. Each forecast covers 48 hours in total.\n- `temperature` - The air temperature at 2 meters above ground in degrees Celsius.\n- `dewpoint` - The dew point temperature at 2 meters above ground in degrees Celsius.\n- `cloudcover_[low/mid/high/total]` - The percentage of the sky covered by clouds in the following altitude bands: 0-2 km, 2-6, 6+, and total.\n- `10_metre_[u/v]_wind_component` - The \\[eastward/northward\\] component of wind speed measured 10 meters above surface in meters per second.\n- `data_block_id`\n- `forecast_datetime` - The timestamp of the predicted weather. Generated from `origin_datetime` plus `hours_ahead`.\n- `direct_solar_radiation` - The direct solar radiation reaching the surface on a plane perpendicular to the direction of the Sun accumulated during the preceding hour, in watt-hours per square meter.\n- `surface_solar_radiation_downwards` - The solar radiation, both direct and diffuse, that reaches a horizontal plane at the surface of the Earth, in watt-hours per square meter.\n- `snowfall` - Snowfall over the previous hour in units of meters of water equivalent.\n- `total_precipitation` - The accumulated liquid, comprising rain and snow that falls on Earth's surface over the preceding hour, in units of meters.\n\n**historical\\_weather.csv** [Historic weather data](https://open-meteo.com/en/docs).\n\n- `datetime`\n- `temperature`\n- `dewpoint`\n- `rain` - Different from the forecast conventions. The rain from large scale weather systems of the preceding hour in millimeters.\n- `snowfall` - Different from the forecast conventions. Snowfall over the preceding hour in centimeters.\n- `surface_pressure` - The air pressure at surface in hectopascals.\n- `cloudcover_[low/mid/high/total]` - Different from the forecast conventions. Cloud cover at 0-3 km, 3-8, 8+, and total.\n- `windspeed_10m` - Different from the forecast conventions. The wind speed at 10 meters above ground in meters per second.\n- `winddirection_10m` - Different from the forecast conventions. The wind direction at 10 meters above ground in degrees.\n- `shortwave_radiation` - Different from the forecast conventions. The global horizontal irradiation in watt-hours per square meter.\n- `direct_solar_radiation`\n- `diffuse_radiation` - Different from the forecast conventions. The diffuse solar irradiation in watt-hours per square meter.\n- `[latitude/longitude]` - The coordinates of the weather station.\n- `data_block_id`\n\n**public\\_timeseries\\_testing\\_util.py** An optional file intended to make it easier to run custom offline API tests. See the script's docstring for details. You will need to edit this file before using it.\n\n**example\\_test\\_files/** Data intended to illustrate how the API functions. Includes the same files and columns delivered by the API. The first three `data_block_ids` are repeats of the last three `data_block_ids` in the train set.\n\n**example\\_test\\_files/sample\\_submission.csv** A valid sample submission, delivered by the API. See [this notebook](https://www.kaggle.com/code/sohier/enefit-basic-submission-demo/notebook) for a very simple example of how to use the sample submission.\n\n**example\\_test\\_files/revealed\\_targets.csv** The actual target values, served with a lag of one day.\n\n**enefit/** Files that enable the API. Expect the API to deliver all rows in under 15 minutes and to reserve less than 0.5 GB of memory. The copy of the API that you can download serves the data from **example\\_test\\_files/**. You must make predictions for those dates in order to advance the API but those predictions are not scored. Expect to see roughly three months of data delivered initially and up to ten months of data by the end of the forecasting period.","metadata":{}},{"cell_type":"markdown","source":"# Install & imports","metadata":{}},{"cell_type":"code","source":"#General\nimport pandas as pd\nimport numpy as np\nimport json\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom colorama import Fore, Style, init;\n\n# Modeling\nimport xgboost as xgb\nimport lightgbm as lgb\nimport torch\n\n# Geolocation\nfrom geopy.geocoders import Nominatim\n\n# Options\npd.set_option('display.max_columns', 100)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T08:03:09.976009Z","iopub.execute_input":"2023-12-12T08:03:09.976840Z","iopub.status.idle":"2023-12-12T08:03:17.364518Z","shell.execute_reply.started":"2023-12-12T08:03:09.976797Z","shell.execute_reply":"2023-12-12T08:03:17.363691Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"DEBUG = False # False/True","metadata":{"execution":{"iopub.status.busy":"2023-12-12T08:03:29.237495Z","iopub.execute_input":"2023-12-12T08:03:29.238651Z","iopub.status.idle":"2023-12-12T08:03:29.242799Z","shell.execute_reply.started":"2023-12-12T08:03:29.238614Z","shell.execute_reply":"2023-12-12T08:03:29.241786Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# GPU or CPU use for model\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-12-12T08:03:30.347039Z","iopub.execute_input":"2023-12-12T08:03:30.348061Z","iopub.status.idle":"2023-12-12T08:03:30.389061Z","shell.execute_reply.started":"2023-12-12T08:03:30.348012Z","shell.execute_reply":"2023-12-12T08:03:30.388095Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Helper functions\ndef display_df(df, name):\n    '''Display df shape and first row '''\n    PrintColor(text = f'{name} data has {df.shape[0]} rows and {df.shape[1]} columns. \\n ===> First row:')\n    display(df.head(1))\n\n# Color printing    \ndef PrintColor(text:str, color = Fore.BLUE, style = Style.BRIGHT):\n    '''Prints color outputs using colorama of a text string'''\n    print(style + color + text + Style.RESET_ALL); ","metadata":{"execution":{"iopub.status.busy":"2023-12-12T08:03:31.827970Z","iopub.execute_input":"2023-12-12T08:03:31.828357Z","iopub.status.idle":"2023-12-12T08:03:31.835313Z","shell.execute_reply.started":"2023-12-12T08:03:31.828324Z","shell.execute_reply":"2023-12-12T08:03:31.833994Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/predict-energy-behavior-of-prosumers/\"\n\n# Read CSVs and parse relevant date columns\ntrain = pd.read_csv(DATA_DIR + \"train.csv\")\nclient = pd.read_csv(DATA_DIR + \"client.csv\")\nhistorical_weather = pd.read_csv(DATA_DIR + \"historical_weather.csv\")\nforecast_weather = pd.read_csv(DATA_DIR + \"forecast_weather.csv\")\nelectricity = pd.read_csv(DATA_DIR + \"electricity_prices.csv\")\ngas = pd.read_csv(DATA_DIR + \"gas_prices.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-12-12T08:03:33.377036Z","iopub.execute_input":"2023-12-12T08:03:33.377956Z","iopub.status.idle":"2023-12-12T08:03:57.325852Z","shell.execute_reply.started":"2023-12-12T08:03:33.377922Z","shell.execute_reply":"2023-12-12T08:03:57.325018Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Location from https://www.kaggle.com/datasets/michaelo/fabiendaniels-mapping-locations-and-county-codes/data\nlocation = (pd.read_csv(\"/kaggle/input/fabiendaniels-mapping-locations-and-county-codes/county_lon_lats.csv\")\n            .drop(columns = [\"Unnamed: 0\"])\n           )","metadata":{"execution":{"iopub.status.busy":"2023-12-12T08:04:04.785794Z","iopub.execute_input":"2023-12-12T08:04:04.786169Z","iopub.status.idle":"2023-12-12T08:04:04.808215Z","shell.execute_reply.started":"2023-12-12T08:04:04.786136Z","shell.execute_reply":"2023-12-12T08:04:04.807364Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"display_df(train, 'train')\ndisplay_df(client, 'client')\ndisplay_df(historical_weather, 'historical weather')\ndisplay_df(forecast_weather, 'forecast weather')\ndisplay_df(electricity, 'electricity prices')\ndisplay_df(gas, 'gas prices')\ndisplay_df(location, 'location data')","metadata":{"execution":{"iopub.status.busy":"2023-12-12T08:04:06.587134Z","iopub.execute_input":"2023-12-12T08:04:06.587916Z","iopub.status.idle":"2023-12-12T08:04:06.660482Z","shell.execute_reply.started":"2023-12-12T08:04:06.587881Z","shell.execute_reply":"2023-12-12T08:04:06.659452Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\u001b[1m\u001b[34mtrain data has 2018352 rows and 9 columns. \n ===> First row:\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   county  is_business  product_type  target  is_consumption  \\\n0       0            0             1   0.713               0   \n\n              datetime  data_block_id  row_id  prediction_unit_id  \n0  2021-09-01 00:00:00              0       0                   0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>county</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>target</th>\n      <th>is_consumption</th>\n      <th>datetime</th>\n      <th>data_block_id</th>\n      <th>row_id</th>\n      <th>prediction_unit_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.713</td>\n      <td>0</td>\n      <td>2021-09-01 00:00:00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\u001b[1m\u001b[34mclient data has 41919 rows and 7 columns. \n ===> First row:\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   product_type  county  eic_count  installed_capacity  is_business  \\\n0             1       0        108              952.89            0   \n\n         date  data_block_id  \n0  2021-09-01              2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_type</th>\n      <th>county</th>\n      <th>eic_count</th>\n      <th>installed_capacity</th>\n      <th>is_business</th>\n      <th>date</th>\n      <th>data_block_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>108</td>\n      <td>952.89</td>\n      <td>0</td>\n      <td>2021-09-01</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\u001b[1m\u001b[34mhistorical weather data has 1710802 rows and 18 columns. \n ===> First row:\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"              datetime  temperature  dewpoint  rain  snowfall  \\\n0  2021-09-01 00:00:00         14.2      11.6   0.0       0.0   \n\n   surface_pressure  cloudcover_total  cloudcover_low  cloudcover_mid  \\\n0            1015.9                31              31               0   \n\n   cloudcover_high  windspeed_10m  winddirection_10m  shortwave_radiation  \\\n0               11       7.083333                  8                  0.0   \n\n   direct_solar_radiation  diffuse_radiation  latitude  longitude  \\\n0                     0.0                0.0      57.6       21.7   \n\n   data_block_id  \n0            1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>datetime</th>\n      <th>temperature</th>\n      <th>dewpoint</th>\n      <th>rain</th>\n      <th>snowfall</th>\n      <th>surface_pressure</th>\n      <th>cloudcover_total</th>\n      <th>cloudcover_low</th>\n      <th>cloudcover_mid</th>\n      <th>cloudcover_high</th>\n      <th>windspeed_10m</th>\n      <th>winddirection_10m</th>\n      <th>shortwave_radiation</th>\n      <th>direct_solar_radiation</th>\n      <th>diffuse_radiation</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>data_block_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-09-01 00:00:00</td>\n      <td>14.2</td>\n      <td>11.6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1015.9</td>\n      <td>31</td>\n      <td>31</td>\n      <td>0</td>\n      <td>11</td>\n      <td>7.083333</td>\n      <td>8</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>57.6</td>\n      <td>21.7</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\u001b[1m\u001b[34mforecast weather data has 3424512 rows and 18 columns. \n ===> First row:\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   latitude  longitude            origin_datetime  hours_ahead  temperature  \\\n0      57.6       21.7  2021-08-31 23:00:00+00:00            1    15.655786   \n\n    dewpoint  cloudcover_high  cloudcover_low  cloudcover_mid  \\\n0  11.553613         0.904816        0.019714             0.0   \n\n   cloudcover_total  10_metre_u_wind_component  10_metre_v_wind_component  \\\n0          0.905899                  -0.411328                  -9.106137   \n\n   data_block_id          forecast_datetime  direct_solar_radiation  \\\n0              1  2021-09-01 00:00:00+00:00                     0.0   \n\n   surface_solar_radiation_downwards  snowfall  total_precipitation  \n0                                0.0       0.0                  0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>origin_datetime</th>\n      <th>hours_ahead</th>\n      <th>temperature</th>\n      <th>dewpoint</th>\n      <th>cloudcover_high</th>\n      <th>cloudcover_low</th>\n      <th>cloudcover_mid</th>\n      <th>cloudcover_total</th>\n      <th>10_metre_u_wind_component</th>\n      <th>10_metre_v_wind_component</th>\n      <th>data_block_id</th>\n      <th>forecast_datetime</th>\n      <th>direct_solar_radiation</th>\n      <th>surface_solar_radiation_downwards</th>\n      <th>snowfall</th>\n      <th>total_precipitation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>57.6</td>\n      <td>21.7</td>\n      <td>2021-08-31 23:00:00+00:00</td>\n      <td>1</td>\n      <td>15.655786</td>\n      <td>11.553613</td>\n      <td>0.904816</td>\n      <td>0.019714</td>\n      <td>0.0</td>\n      <td>0.905899</td>\n      <td>-0.411328</td>\n      <td>-9.106137</td>\n      <td>1</td>\n      <td>2021-09-01 00:00:00+00:00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\u001b[1m\u001b[34melectricity prices data has 15286 rows and 4 columns. \n ===> First row:\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"         forecast_date  euros_per_mwh          origin_date  data_block_id\n0  2021-09-01 00:00:00          92.51  2021-08-31 00:00:00              1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>forecast_date</th>\n      <th>euros_per_mwh</th>\n      <th>origin_date</th>\n      <th>data_block_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-09-01 00:00:00</td>\n      <td>92.51</td>\n      <td>2021-08-31 00:00:00</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\u001b[1m\u001b[34mgas prices data has 637 rows and 5 columns. \n ===> First row:\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  forecast_date  lowest_price_per_mwh  highest_price_per_mwh origin_date  \\\n0    2021-09-01                 45.23                  46.32  2021-08-31   \n\n   data_block_id  \n0              1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>forecast_date</th>\n      <th>lowest_price_per_mwh</th>\n      <th>highest_price_per_mwh</th>\n      <th>origin_date</th>\n      <th>data_block_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-09-01</td>\n      <td>45.23</td>\n      <td>46.32</td>\n      <td>2021-08-31</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\u001b[1m\u001b[34mlocation data data has 75 rows and 3 columns. \n ===> First row:\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   county  longitude  latitude\n0       0       24.2      59.1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>county</th>\n      <th>longitude</th>\n      <th>latitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>24.2</td>\n      <td>59.1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# See county codes\nwith open(DATA_DIR + 'county_id_to_name_map.json') as f:\n    county_codes = json.load(f)\npd.DataFrame(county_codes, index=[0])","metadata":{"execution":{"iopub.status.busy":"2023-12-12T08:04:14.547236Z","iopub.execute_input":"2023-12-12T08:04:14.547643Z","iopub.status.idle":"2023-12-12T08:04:14.571526Z","shell.execute_reply.started":"2023-12-12T08:04:14.547611Z","shell.execute_reply":"2023-12-12T08:04:14.570516Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"          0        1            2         3          4              5  \\\n0  HARJUMAA  HIIUMAA  IDA-VIRUMAA  J√ÑRVAMAA  J√ïGEVAMAA  L√Ñ√ÑNE-VIRUMAA   \n\n          6         7         8         9        10        11       12  \\\n0  L√Ñ√ÑNEMAA  P√ÑRNUMAA  P√ïLVAMAA  RAPLAMAA  SAAREMAA  TARTUMAA  UNKNOWN   \n\n         13           14       15  \n0  VALGAMAA  VILJANDIMAA  V√ïRUMAA  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HARJUMAA</td>\n      <td>HIIUMAA</td>\n      <td>IDA-VIRUMAA</td>\n      <td>J√ÑRVAMAA</td>\n      <td>J√ïGEVAMAA</td>\n      <td>L√Ñ√ÑNE-VIRUMAA</td>\n      <td>L√Ñ√ÑNEMAA</td>\n      <td>P√ÑRNUMAA</td>\n      <td>P√ïLVAMAA</td>\n      <td>RAPLAMAA</td>\n      <td>SAAREMAA</td>\n      <td>TARTUMAA</td>\n      <td>UNKNOWN</td>\n      <td>VALGAMAA</td>\n      <td>VILJANDIMAA</td>\n      <td>V√ïRUMAA</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# EAD & Data processing","metadata":{}},{"cell_type":"code","source":"class FeatureProcessorClass():\n    def __init__(self):         \n        # Columns to join on for the different datasets\n        self.weather_join = ['datetime', 'county', 'data_block_id']\n        self.gas_join = ['data_block_id']\n        self.electricity_join = ['datetime', 'data_block_id']\n        self.client_join = ['county', 'is_business', 'product_type', 'data_block_id']\n        \n        # Columns of latitude & longitude\n        self.lat_lon_columns = ['latitude', 'longitude']\n        \n        # Aggregate stats \n        self.agg_stats = ['mean'] #, 'min', 'max', 'std', 'median']\n        \n        # Categorical columns (specify for XGBoost)\n        self.category_columns = ['county', 'is_business', 'product_type', 'is_consumption', 'data_block_id']\n\n    def create_new_column_names(self, df, suffix, columns_no_change):\n        '''Change column names by given suffix, keep columns_no_change, and return back the data'''\n        df.columns = [col + suffix \n                      if col not in columns_no_change\n                      else col\n                      for col in df.columns\n                      ]\n        return df \n\n    def flatten_multi_index_columns(self, df):\n        df.columns = ['_'.join([col for col in multi_col if len(col)>0]) \n                      for multi_col in df.columns]\n        return df\n    \n    def create_data_features(self, data):\n        '''üìäCreate features for main data (test or train) setüìä'''\n        # To datetime\n        data['datetime'] = pd.to_datetime(data['datetime'])\n        \n        # Time period features\n        data['date'] = data['datetime'].dt.normalize()\n        data['year'] = data['datetime'].dt.year\n        data['quarter'] = data['datetime'].dt.quarter\n        data['month'] = data['datetime'].dt.month\n        data['week'] = data['datetime'].dt.isocalendar().week\n        data['hour'] = data['datetime'].dt.hour\n        \n        # Day features\n        data['day_of_year'] = data['datetime'].dt.day_of_year\n        data['day_of_month']  = data['datetime'].dt.day\n        data['day_of_week'] = data['datetime'].dt.day_of_week\n        return data\n\n    def create_client_features(self, client):\n        '''üíº Create client features üíº'''\n        # Modify column names - specify suffix\n        client = self.create_new_column_names(client, \n                                           suffix='_client',\n                                           columns_no_change = self.client_join\n                                          )       \n        return client\n    \n    def create_historical_weather_features(self, historical_weather):\n        '''‚åõüå§Ô∏è Create historical weather features üå§Ô∏è‚åõ'''\n        \n        # To datetime\n        historical_weather['datetime'] = pd.to_datetime(historical_weather['datetime'])\n        \n        # Add county\n        historical_weather[self.lat_lon_columns] = historical_weather[self.lat_lon_columns].astype(float).round(1)\n        historical_weather = historical_weather.merge(location, how = 'left', on = self.lat_lon_columns)\n\n        # Modify column names - specify suffix\n        historical_weather = self.create_new_column_names(historical_weather,\n                                                          suffix='_h',\n                                                          columns_no_change = self.lat_lon_columns + self.weather_join\n                                                          ) \n        \n        # Group by & calculate aggregate stats \n        agg_columns = [col for col in historical_weather.columns if col not in self.lat_lon_columns + self.weather_join]\n        agg_dict = {agg_col: self.agg_stats for agg_col in agg_columns}\n        historical_weather = historical_weather.groupby(self.weather_join).agg(agg_dict).reset_index() \n        \n        # Flatten the multi column aggregates\n        historical_weather = self.flatten_multi_index_columns(historical_weather) \n        \n        # Test set has 1 day offset for hour<11 and 2 day offset for hour>11\n        historical_weather['hour_h'] = historical_weather['datetime'].dt.hour\n        historical_weather['datetime'] = (historical_weather\n                                               .apply(lambda x: \n                                                      x['datetime'] + pd.DateOffset(1) \n                                                      if x['hour_h']< 11 \n                                                      else x['datetime'] + pd.DateOffset(2),\n                                                      axis=1)\n                                              )\n        \n        return historical_weather\n    \n    def create_forecast_weather_features(self, forecast_weather):\n        '''üîÆüå§Ô∏è Create forecast weather features üå§Ô∏èüîÆ'''\n        \n        # Rename column and drop\n        forecast_weather = (forecast_weather\n                            .rename(columns = {'forecast_datetime': 'datetime'})\n                            .drop(columns = 'origin_datetime') # not needed\n                           )\n        \n        # To datetime\n        forecast_weather['datetime'] = (pd.to_datetime(forecast_weather['datetime'])\n                                        .dt\n                                        .tz_convert('Europe/Brussels') # change to different time zone?\n                                        .dt\n                                        .tz_localize(None)\n                                       )\n\n        # Add county\n        forecast_weather[self.lat_lon_columns] = forecast_weather[self.lat_lon_columns].astype(float).round(1)\n        forecast_weather = forecast_weather.merge(location, how = 'left', on = self.lat_lon_columns)\n        \n        # Modify column names - specify suffix\n        forecast_weather = self.create_new_column_names(forecast_weather,\n                                                        suffix='_f',\n                                                        columns_no_change = self.lat_lon_columns + self.weather_join\n                                                        ) \n        \n        # Group by & calculate aggregate stats \n        agg_columns = [col for col in forecast_weather.columns if col not in self.lat_lon_columns + self.weather_join]\n        agg_dict = {agg_col: self.agg_stats for agg_col in agg_columns}\n        forecast_weather = forecast_weather.groupby(self.weather_join).agg(agg_dict).reset_index() \n        \n        # Flatten the multi column aggregates\n        forecast_weather = self.flatten_multi_index_columns(forecast_weather)     \n        return forecast_weather\n\n    def create_electricity_features(self, electricity):\n        '''‚ö° Create electricity prices features ‚ö°'''\n        # To datetime\n        electricity['forecast_date'] = pd.to_datetime(electricity['forecast_date'])\n        \n        # Test set has 1 day offset\n        electricity['datetime'] = electricity['forecast_date'] + pd.DateOffset(1)\n        \n        # Modify column names - specify suffix\n        electricity = self.create_new_column_names(electricity, \n                                                   suffix='_electricity',\n                                                   columns_no_change = self.electricity_join\n                                                  )             \n        return electricity\n\n    def create_gas_features(self, gas):\n        '''‚õΩ Create gas prices features ‚õΩ'''\n        # Mean gas price\n        gas['mean_price_per_mwh'] = (gas['lowest_price_per_mwh'] + gas['highest_price_per_mwh'])/2\n        \n        # Modify column names - specify suffix\n        gas = self.create_new_column_names(gas, \n                                           suffix='_gas',\n                                           columns_no_change = self.gas_join\n                                          )       \n        return gas\n    \n    def __call__(self, data, client, historical_weather, forecast_weather, electricity, gas):\n        '''Processing of features from all datasets, merge together and return features for dataframe df '''\n        # Create features for relevant dataset\n        data = self.create_data_features(data)\n        client = self.create_client_features(client)\n        historical_weather = self.create_historical_weather_features(historical_weather)\n        forecast_weather = self.create_forecast_weather_features(forecast_weather)\n        electricity = self.create_electricity_features(electricity)\n        gas = self.create_gas_features(gas)\n        \n        # üîó Merge all datasets into one df üîó\n        df = data.merge(client, how='left', on = self.client_join)\n        df = df.merge(historical_weather, how='left', on = self.weather_join)\n        df = df.merge(forecast_weather, how='left', on = self.weather_join)\n        df = df.merge(electricity, how='left', on = self.electricity_join)\n        df = df.merge(gas, how='left', on = self.gas_join)\n        \n        # Change columns to categorical for XGBoost\n        df[self.category_columns] = df[self.category_columns].astype('category')\n        return df","metadata":{"execution":{"iopub.status.busy":"2023-12-12T08:08:31.172047Z","iopub.execute_input":"2023-12-12T08:08:31.172389Z","iopub.status.idle":"2023-12-12T08:08:31.202397Z","shell.execute_reply.started":"2023-12-12T08:08:31.172362Z","shell.execute_reply":"2023-12-12T08:08:31.201361Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def create_revealed_targets_train(data, N_day_lags):\n    '''üéØ Create past revealed_targets for train set based on number of day lags N_day_lags üéØ '''    \n    original_datetime = data['datetime']\n    revealed_targets = data[['datetime', 'prediction_unit_id', 'is_consumption', 'target']].copy()\n    \n    # Create revealed targets for all day lags\n    for day_lag in range(2, N_day_lags+1):\n        revealed_targets['datetime'] = original_datetime + pd.DateOffset(day_lag)\n        data = data.merge(revealed_targets, \n                          how='left', \n                          on = ['datetime', 'prediction_unit_id', 'is_consumption'],\n                          suffixes = ('', f'_{day_lag}_days_ago')\n                         )\n    return data","metadata":{"execution":{"iopub.status.busy":"2023-12-12T08:08:35.528569Z","iopub.execute_input":"2023-12-12T08:08:35.529491Z","iopub.status.idle":"2023-12-12T08:08:35.535712Z","shell.execute_reply.started":"2023-12-12T08:08:35.529456Z","shell.execute_reply":"2023-12-12T08:08:35.534742Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"%%time\n# Create all features\nN_day_lags = 15 # Specify how many days we want to go back (at least 2)\n\nFeatureProcessor = FeatureProcessorClass()\n\ndata = FeatureProcessor(data = train.copy(),\n                      client = client.copy(),\n                      historical_weather = historical_weather.copy(),\n                      forecast_weather = forecast_weather.copy(),\n                      electricity = electricity.copy(),\n                      gas = gas.copy(),\n                     )\n\ndf = create_revealed_targets_train(data.copy(), \n                                  N_day_lags = N_day_lags)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T08:08:37.747631Z","iopub.execute_input":"2023-12-12T08:08:37.748357Z","iopub.status.idle":"2023-12-12T08:09:10.551770Z","shell.execute_reply.started":"2023-12-12T08:08:37.748303Z","shell.execute_reply":"2023-12-12T08:09:10.550491Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"CPU times: user 29.6 s, sys: 3.12 s, total: 32.8 s\nWall time: 32.8 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# XGBoost single fold","metadata":{}},{"cell_type":"code","source":"#### Create single fold split ######\n# Remove empty target row\ntarget = 'target'\ndf = df[df[target].notnull()].reset_index(drop=True)\n\ntrain_block_id = list(range(0, 600)) \n\ntr = df[df['data_block_id'].isin(train_block_id)] # first 600 data_block_ids used for training\nval = df[~df['data_block_id'].isin(train_block_id)] # rest data_block_ids used for validation","metadata":{"execution":{"iopub.status.busy":"2023-11-11T09:53:52.032226Z","iopub.execute_input":"2023-11-11T09:53:52.032574Z","iopub.status.idle":"2023-11-11T09:53:54.098482Z","shell.execute_reply.started":"2023-11-11T09:53:52.032543Z","shell.execute_reply":"2023-11-11T09:53:54.097373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove columns for features\nno_features = ['date', \n                'latitude', \n                'longitude', \n                'data_block_id', \n                'row_id',\n                'hours_ahead',\n                'hour_h',\n               ]\n\nremove_columns = [col for col in df.columns for no_feature in no_features if no_feature in col]\nremove_columns.append(target)\nfeatures = [col for col in df.columns if col not in remove_columns]\nPrintColor(f'There are {len(features)} features: {features}')","metadata":{"execution":{"iopub.status.busy":"2023-11-11T09:53:54.099809Z","iopub.execute_input":"2023-11-11T09:53:54.100126Z","iopub.status.idle":"2023-11-11T09:53:54.106917Z","shell.execute_reply.started":"2023-11-11T09:53:54.1001Z","shell.execute_reply":"2023-11-11T09:53:54.106008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = xgb.XGBRegressor(\n                        device = device,\n                        enable_categorical=True,\n                        objective = 'reg:absoluteerror',\n                        n_estimators = 2 if DEBUG else 1500,\n                        early_stopping_rounds=100\n                       )","metadata":{"execution":{"iopub.status.busy":"2023-11-11T09:53:54.108149Z","iopub.execute_input":"2023-11-11T09:53:54.108504Z","iopub.status.idle":"2023-11-11T09:53:54.121105Z","shell.execute_reply.started":"2023-11-11T09:53:54.108478Z","shell.execute_reply":"2023-11-11T09:53:54.120111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.fit(X = tr[features], \n        y = tr[target], \n        eval_set = [(tr[features], tr[target]), (val[features], val[target])], \n        verbose=True #False #True\n       )","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-11T09:53:54.122276Z","iopub.execute_input":"2023-11-11T09:53:54.122558Z","iopub.status.idle":"2023-11-11T09:55:03.666305Z","shell.execute_reply.started":"2023-11-11T09:53:54.122533Z","shell.execute_reply":"2023-11-11T09:55:03.665352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PrintColor(f'Early stopping on best iteration #{clf.best_iteration} with MAE error on validation set of {clf.best_score:.2f}')","metadata":{"execution":{"iopub.status.busy":"2023-11-11T09:55:03.667613Z","iopub.execute_input":"2023-11-11T09:55:03.667946Z","iopub.status.idle":"2023-11-11T09:55:03.673349Z","shell.execute_reply.started":"2023-11-11T09:55:03.667919Z","shell.execute_reply":"2023-11-11T09:55:03.672221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot RMSE\nresults = clf.evals_result()\ntrain_mae, val_mae = results[\"validation_0\"][\"mae\"], results[\"validation_1\"][\"mae\"]\nx_values = range(0, len(train_mae))\nfig, ax = plt.subplots(figsize=(8,4))\nax.plot(x_values, train_mae, label=\"Train MAE\")\nax.plot(x_values, val_mae, label=\"Validation MAE\")\nax.legend()\nplt.ylabel(\"MAE Loss\")\nplt.title(\"XGBoost MAE Loss\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T09:55:03.674645Z","iopub.execute_input":"2023-11-11T09:55:03.674952Z","iopub.status.idle":"2023-11-11T09:55:04.036349Z","shell.execute_reply.started":"2023-11-11T09:55:03.674926Z","shell.execute_reply":"2023-11-11T09:55:04.03529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TOP = 20\nimportance_data = pd.DataFrame({'name': clf.feature_names_in_, 'importance': clf.feature_importances_})\nimportance_data = importance_data.sort_values(by='importance', ascending=False)\n\nfig, ax = plt.subplots(figsize=(8,4))\nsns.barplot(data=importance_data[:TOP],\n            x = 'importance',\n            y = 'name'\n        )\npatches = ax.patches\ncount = 0\nfor patch in patches:\n    height = patch.get_height() \n    width = patch.get_width()\n    perc = 100*importance_data['importance'].iloc[count]#100*width/len(importance_data)\n    ax.text(width, patch.get_y() + height/2, f'{perc:.1f}%')\n    count+=1\n    \nplt.title(f'The top {TOP} features sorted by importance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T09:55:04.037679Z","iopub.execute_input":"2023-11-11T09:55:04.038068Z","iopub.status.idle":"2023-11-11T09:55:04.581313Z","shell.execute_reply.started":"2023-11-11T09:55:04.038031Z","shell.execute_reply":"2023-11-11T09:55:04.580229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance_data[importance_data['importance']<0.0005].name.values","metadata":{"execution":{"iopub.status.busy":"2023-11-11T09:55:04.582558Z","iopub.execute_input":"2023-11-11T09:55:04.582899Z","iopub.status.idle":"2023-11-11T09:55:04.591417Z","shell.execute_reply.started":"2023-11-11T09:55:04.582869Z","shell.execute_reply":"2023-11-11T09:55:04.589991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"def create_revealed_targets_test(data, previous_revealed_targets, N_day_lags):\n    '''üéØ Create new test data based on previous_revealed_targets and N_day_lags üéØ ''' \n    for count, revealed_targets in enumerate(previous_revealed_targets) :\n        day_lag = count + 2\n        \n        # Get hour\n        revealed_targets['hour'] = pd.to_datetime(revealed_targets['datetime']).dt.hour\n        \n        # Select columns and rename target\n        revealed_targets = revealed_targets[['hour', 'prediction_unit_id', 'is_consumption', 'target']]\n        revealed_targets = revealed_targets.rename(columns = {\"target\" : f\"target_{day_lag}_days_ago\"})\n        \n        \n        # Add past revealed targets\n        data = pd.merge(data,\n                        revealed_targets,\n                        how = 'left',\n                        on = ['hour', 'prediction_unit_id', 'is_consumption'],\n                       )\n        \n    # If revealed_target_columns not available, replace by nan\n    all_revealed_columns = [f\"target_{day_lag}_days_ago\" for day_lag in range(2, N_day_lags+1)]\n    missing_columns = list(set(all_revealed_columns) - set(data.columns))\n    data[missing_columns] = np.nan \n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2023-11-10T19:03:59.09856Z","iopub.execute_input":"2023-11-10T19:03:59.098813Z","iopub.status.idle":"2023-11-10T19:03:59.108117Z","shell.execute_reply.started":"2023-11-10T19:03:59.09879Z","shell.execute_reply":"2023-11-10T19:03:59.107261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import enefit\nenv = enefit.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2023-11-11T09:44:15.800119Z","iopub.execute_input":"2023-11-11T09:44:15.800654Z","iopub.status.idle":"2023-11-11T09:44:15.82874Z","shell.execute_reply.started":"2023-11-11T09:44:15.800616Z","shell.execute_reply":"2023-11-11T09:44:15.827599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reload enefit environment (only in debug mode, otherwise the submission will fail)\nif DEBUG:\n    enefit.make_env.__called__ = False\n    type(env)._state = type(type(env)._state).__dict__['INIT']\n    iter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2023-11-10T19:05:21.552472Z","iopub.execute_input":"2023-11-10T19:05:21.552911Z","iopub.status.idle":"2023-11-10T19:05:21.558274Z","shell.execute_reply.started":"2023-11-10T19:05:21.552879Z","shell.execute_reply":"2023-11-10T19:05:21.557217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of target_revealed dataframes\nprevious_revealed_targets = []\n\nfor (test, \n     revealed_targets, \n     client_test, \n     historical_weather_test,\n     forecast_weather_test, \n     electricity_test, \n     gas_test, \n     sample_prediction) in iter_test:\n    \n    # Rename test set to make consistent with train\n    test = test.rename(columns = {'prediction_datetime': 'datetime'})\n\n    # Initiate column data_block_id with default value to join on\n    id_column = 'data_block_id' \n    \n    test[id_column] = 0\n    gas_test[id_column] = 0\n    electricity_test[id_column] = 0\n    historical_weather_test[id_column] = 0\n    forecast_weather_test[id_column] = 0\n    client_test[id_column] = 0\n    revealed_targets[id_column] = 0\n    \n    data_test = FeatureProcessor(\n                               data = test,\n                               client = client_test, \n                               historical_weather = historical_weather_test,\n                               forecast_weather = forecast_weather_test, \n                               electricity = electricity_test, \n                               gas = gas_test\n                               )\n    \n    # Store revealed_targets\n    previous_revealed_targets.insert(0, revealed_targets)\n    \n    if len(previous_revealed_targets) == N_day_lags:\n        previous_revealed_targets.pop()\n    \n    # Add previous revealed targets\n    df_test = create_revealed_targets_test(data = data_test.copy(),\n                                           previous_revealed_targets = previous_revealed_targets.copy(),\n                                           N_day_lags = N_day_lags\n                                          )\n    \n    # Make prediction\n    X_test = df_test[features]\n    sample_prediction['target'] = clf.predict(X_test)\n    env.predict(sample_prediction)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T19:05:23.449105Z","iopub.execute_input":"2023-11-10T19:05:23.450075Z","iopub.status.idle":"2023-11-10T19:05:23.459128Z","shell.execute_reply.started":"2023-11-10T19:05:23.450032Z","shell.execute_reply":"2023-11-10T19:05:23.458175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Next steps\n> üìå**Note**: If you liked or forked this notebook, please consider upvoting ‚¨ÜÔ∏è‚¨ÜÔ∏è It encourages to keep posting relevant content. Feedback is always welcome!!\n\n<div style=\"border-radius:10px; border: #babab5 solid; padding: 15px; background-color: #e6f9ff; font-size:100%;\">\n\n* Create more rolling / lag features and make sure they are robust on the test set\n* Be creative with new feature engineering\n* Cross validation and hyperparameter tuning\n* Choose other models e.g. CatBoost, LGBM, Neural Networks (Transformers?) and ensemble  \n* Alternative merging, not sure the merging I used is the most correct!","metadata":{}}]}